# Cloud Build configuration for deploying Ollama with GPU on Cloud Run
steps:
  # Build Ollama container with embedded model
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-f'
      - 'Dockerfile.ollama'
      - '-t'
      - 'gcr.io/$PROJECT_ID/ollama-qwen:$SHORT_SHA'
      - '-t'
      - 'gcr.io/$PROJECT_ID/ollama-qwen:latest'
      - '.'

  # Push to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/$PROJECT_ID/ollama-qwen:$SHORT_SHA'

  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/$PROJECT_ID/ollama-qwen:latest'

  # Deploy to Cloud Run with GPU
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'ollama-qwen'
      - '--image=gcr.io/$PROJECT_ID/ollama-qwen:$SHORT_SHA'
      - '--region=$_REGION'
      - '--platform=managed'
      - '--gpu=1'
      - '--gpu-type=nvidia-l4'
      - '--cpu=8'
      - '--memory=32Gi'
      - '--timeout=600'
      - '--max-instances=1'
      - '--min-instances=0'
      - '--concurrency=4'
      - '--allow-unauthenticated=false'
      - '--set-env-vars=OLLAMA_HOST=0.0.0.0:8080,OLLAMA_MODELS=/models,OLLAMA_KEEP_ALIVE=-1,OLLAMA_NUM_PARALLEL=4'

images:
  - 'gcr.io/$PROJECT_ID/ollama-qwen:$SHORT_SHA'
  - 'gcr.io/$PROJECT_ID/ollama-qwen:latest'

substitutions:
  _REGION: us-central1

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'N1_HIGHCPU_8'

timeout: 2400s  # 40 minutes for large model download
